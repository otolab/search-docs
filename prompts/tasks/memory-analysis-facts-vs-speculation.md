# メモリ増加分析 - 事実と推測の分離

## 1. 観測された事実（FACTS）

### 1.1 メモリ使用量の測定データ

#### karte-io-systems プロジェクト (102,893 Markdownファイル)

**測定1: デフォルト設定（mimalloc）**
- 0秒: 起動時
- 60秒: 5.36 GB
- パターン: 急激な増加

**測定2: system memory pool設定**
- 0秒: 24 MB
- 60秒: 2.5 GB (約50%改善)
- 478秒: 6.5 GB
- パターン: 段階的増加、時々急激に増加

**測定3: フレッシュスタート + 1秒間隔監視**
```
時間    メモリ   スレッド数  フェーズ
0s      985MB    32         起動
7s      985MB    32         ファイル検出
18s     1.1GB    33         ファイル検出
28s     3.0GB    69         インデックス作成開始 ★
39s     3.5GB    86
49s     4.1GB    91
60s     4.4GB    93
70s     5.9GB    97
81s     6.2GB    97
100-177s 6.1GB   97         安定
```

**測定4: 100ms wait追加後**
```
時間    メモリ   スレッド数
7s      985MB    32
28s     3.0GB    69
81s     6.2GB    97
```
→ 100ms wait追加による有意な差はなし

### 1.2 スレッド数の測定データ

**フレッシュスタート測定**:
- 起動時: 32スレッド
- ファイル検出中: 32-33スレッド
- インデックス作成開始: 69スレッド (★急増)
- インデックス作成中: 86 → 91 → 93 → 97スレッド
- 安定時: 97スレッド

**system memory pool測定**:
- 最終的に173スレッドまで増加

### 1.3 ユニットテストの結果

**Python worker単体テスト (全てPASSED)**:

| テスト | 実行回数 | オブジェクト増加数 | 1回あたり | 判定 |
|--------|---------|------------------|----------|------|
| add_sections (1x vs 10x) | 10回 | 0個 | 0.00/iter | リークなし |
| search (1x vs 10x) | 10回 | 0個 | 0.00/iter | リークなし |
| get_stats (1x vs 10x) | 10回 | 0個 | 0.00/iter | リークなし |
| add_sections (100回) | 100回 | +466,039個 (初回のみ) | 0/iter (2回目以降) | リークなし |
| search (1000回) | 1000回 | +24個 | 0.02/iter | リークなし |
| get_stats (1000回) | 1000回 | +44個 | 0.04/iter | リークなし |
| LanceDB混合 (1000回) | 1000回 | +56個 | 0.06/iter | リークなし |
| Transformers (1000回) | 1000回 | +45個 | 0.04/iter | リークなし |

**結論**: Pythonコード自体にメモリリークは検出されない

### 1.4 コード構造の事実

**現在の実装**:
```typescript
// index-worker.ts (Line 104-106)
for (const request of requests) {
  await this.processRequest(request);  // 1ファイルごとに処理
}

// processRequest内 (Line 232-236)
await this.dbEngine.addSections(sections);  // 1ファイル = 1回のadd_sections()呼び出し
await new Promise(resolve => setTimeout(resolve, 100));  // 100ms wait
```

**呼び出し回数**:
- karte-io-systems: 102,893ファイル = 約**102,893回のadd_sections()呼び出し**

**LanceDB推奨事項**（公式ドキュメント）:
- バッチ挿入を使用（1行ずつではなく）
- Fragment数を100以下に保つ
- 定期的にcompact_files()実行

### 1.5 LanceDB API仕様

**判明した事実**:
- 明示的な`flush()`、`sync()`メソッドは存在しない
- `table.add()`は各呼び出しで新バージョンを作成
- バージョン管理はLanceDB内部で自動処理
- `optimize.compact_files()`でフラグメント最適化可能

### 1.6 PyArrowメモリプール

**利用可能なバックエンド**:
- mimalloc (デフォルト): Overallocationでパフォーマンス重視
- jemalloc: Overallocationあり
- system (malloc): 標準allocator、overallocation少ない

**LanceDBドキュメントの実測例**:
- mimalloc: 722MB
- jemalloc: 657MB
- 差: 約65MB (9%)

**本調査での実測**:
- mimalloc: 60秒時点 5.36 GB
- system: 60秒時点 2.5 GB
- 差: 約2.86 GB (53%改善)

### 1.7 観測された相関

**スレッド数とメモリ使用量**:
- 28秒: 32→69スレッド (+37) / 985MB→3.0GB (+2.0GB)
- 39秒: 69→86スレッド (+17) / 3.0GB→3.5GB (+0.5GB)
- 49秒: 86→91スレッド (+5) / 3.5GB→4.1GB (+0.6GB)
- 60秒: 91→93スレッド (+2) / 4.1GB→4.4GB (+0.3GB)
- 70秒: 93→97スレッド (+4) / 4.4GB→5.9GB (+1.5GB)
- 81秒: 97→97スレッド (0) / 5.9GB→6.2GB (+0.3GB)

**観察**: スレッド数の急増時にメモリも急増

### 1.8 ユーザーからの観察

**引用**:
> "メモリプレッシャーが強くなってから使用量が下がる動きも観察できました"

**引用**:
> "リニアに増えていない。急に大きく上昇する"

**引用**:
> "いまはスレッドの数に比例している感じがする"

### 1.9 実施した対策とその効果

| 対策 | 効果 |
|------|------|
| `.select(["document_path"])` + `limit=1000` (get_stats) | 未測定（get_statsの呼び出し頻度が低い） |
| テーブルハンドルキャッシュ化 | 未測定 |
| `gc.collect()`をadd_sections後に追加 | ユニットテストでリークなし確認、本番効果未測定 |
| `pa.set_memory_pool(pa.system_memory_pool())` | 60秒時点で53%改善、最終的には同程度 |
| スレッド数制限 (OMP_NUM_THREADS=4等) | 効果不明（スレッド数は97-173まで増加） |
| 100回ごとに`compact_files()`実行 | 効果未測定 |
| add_sections後に100ms wait | 効果なし（メモリ増加パターン同一） |

---

## 2. 推測・仮説（SPECULATION）

### 2.1 メモリ増加の根本原因（未検証）

#### 仮説A: LanceDB Fragment数の爆発
**推測内容**:
- 現在: 102,893回のadd_sections()呼び出し = 最大102,893個のFragment
- LanceDB推奨: Fragment数 < 100
- 推測: Fragment管理のオーバーヘッドがメモリを消費

**根拠**:
- LanceDB公式が「Fragment数 < 100」を推奨
- 現在の実装は1ファイル = 1呼び出し

**未検証の理由**:
- 実際のFragment数を確認していない
- Fragment数とメモリ使用量の関係を測定していない

**検証方法**:
```python
table = db.open_table("sections")
fragments = table.list_versions()  # または類似のAPI
print(f"Fragment count: {len(fragments)}")
```

#### 仮説B: スレッド生成によるオーバーヘッド
**推測内容**:
- スレッド数: 32 → 97-173
- 各スレッドがスタック領域等を消費
- スレッド生成元: PyTorch/Transformers、LanceDB Rust backend、またはTypeScript

**根拠**:
- スレッド数とメモリ増加の時間的相関
- 特に28秒時点での急増（32→69スレッド、+2.0GB）

**未検証の理由**:
- スレッド生成元を特定していない
- スレッド1つあたりのメモリ消費量を測定していない
- スレッド数制限（OMP_NUM_THREADS=4）の効果が不明

**検証方法**:
- スレッドダンプによる生成元特定
- より厳格なスレッド数制限テスト
- スレッドあたりのメモリ消費量測定

#### 仮説C: PyArrow内部バッファの蓄積
**推測内容**:
- PyArrowが内部的にデータをバッファリング
- 102,893回の呼び出しでバッファが蓄積
- system memory poolでもoverallocationが発生

**根拠**:
- system memory pool使用時も最終的に6.5GBまで増加
- PyArrowは大量の小さな書き込みに最適化されていない

**未検証の理由**:
- PyArrow内部の状態を直接観測していない
- バッファフラッシュの有無を確認していない

#### 仮説D: TypeScript-Python通信バッファ
**推測内容**:
- JSON-RPC通信でデータがバッファに蓄積
- 102,893回の呼び出しで通信バッファが蓄積

**根拠**:
- 大量のadd_sections()呼び出し
- ユニットテストでPython単体にリークなし

**未検証の理由**:
- 通信バッファのサイズを測定していない
- TypeScriptプロセスのメモリ使用量を測定していない

**検証方法**:
- TypeScriptプロセスのメモリ監視
- Python単独実行（TypeScript経由なし）との比較

### 2.2 100ms wait が効果なかった理由（推測）

**ユーザーの指摘**:
> "addをまとめるように言ったつもりだったんだけど。"

**私の誤解**:
- ユーザーの意図: 複数ファイルのsectionsをまとめて1回のadd_sections()で追加
- 私の実装: 各add_sections()呼び出し後に100ms待機

**推測される理由**:
- 100ms待機は書き込み完了を待つだけで、呼び出し回数は変わらない
- Fragment数は依然として102,893個のまま
- メモリ使用量は呼び出し回数（Fragment数）に依存すると推測される

### 2.3 急激な増加タイミングの推測

**観測**: 28秒時点で985MB→3.0GB (+2.0GB)

**推測1**: Transformersモデルのロード
- 初回のsection追加時にRuriモデル（cl-nagoya/ruri-v3-30m）をロード
- モデルサイズ + キャッシュで約2GB消費の可能性

**推測2**: LanceDBインデックス構築の閾値
- 一定量のデータが蓄積された時点でインデックスを構築
- インデックス構築時にメモリを大量消費

**推測3**: スレッドプールの拡張
- 32→69スレッド (+37スレッド)
- スレッドプール拡張のタイミングでメモリ確保

**未検証**: いずれも推測であり、実際のタイミングと原因の因果関係は未確認

### 2.4 処理量の自乗に比例する可能性（未検証）

**ユーザーの指摘**:
> "作業量とリーク量が関係している（処理量の自乗に比例）という可能性を示唆しています"

**根拠**:
- 急激な増加パターン
- 段階的な増加

**数学的検証**: 未実施
- N個のファイル処理でメモリ使用量がO(N²)になる要因は未特定
- Fragment管理のオーバーヘッドがO(N²)である可能性

### 2.5 最適な解決策（推測）

#### 推測される解決策1: add_sections()のバッチ化
**内容**:
```typescript
// 現在: 1ファイル = 1呼び出し
for (const request of requests) {
  const sections = this.splitter.split(...);
  await this.dbEngine.addSections(sections);  // 102,893回
}

// 提案: 100ファイルまとめて1呼び出し
const allSections = [];
for (const request of requests) {
  const sections = this.splitter.split(...);
  allSections.push(...sections);

  if (allSections.length >= 10000 || isLastRequest) {  // バッチサイズ閾値
    await this.dbEngine.addSections(allSections);  // 約1,029回
    allSections = [];
  }
}
```

**期待される効果**:
- add_sections()呼び出し回数: 102,893回 → 約1,000回
- Fragment数: 約100,000個 → 約1,000個 (推奨値<100には届かないが大幅改善)
- メモリ使用量: Fragment管理オーバーヘッド削減

**懸念点（未検証）**:
- 1回のバッチが大きすぎる場合のメモリ消費
- エラー時の再処理粒度の粗さ

#### 推測される解決策2: より厳格なスレッド制限
**内容**:
```python
os.environ['OMP_NUM_THREADS'] = '1'
os.environ['MKL_NUM_THREADS'] = '1'
# ... 他のスレッド設定も1に
```

**期待される効果**:
- スレッド数を最小限に抑制
- スレッドあたりのメモリ消費を削減

**懸念点（未検証）**:
- パフォーマンス低下の可能性
- 現在の設定（=4）でも効果不明のため、さらなる制限の効果も不明

#### 推測される解決策3: compact_files()の実行タイミング最適化
**内容**:
現在: 100回ごと
提案: バッチ化と組み合わせ、各バッチ後に実行

**期待される効果**:
- Fragment断片化の即座解消
- メモリ使用量の安定化

**懸念点（未検証）**:
- compact_files()自体のメモリ消費量
- 実行時間の増加

---

## 3. 優先度付き検証項目

### 優先度: 高

1. **LanceDB Fragment数の確認**
   - 現状: 未確認
   - 方法: `table.list_versions()`または類似API
   - 目的: 仮説Aの検証

2. **add_sections()バッチ化の実装と効果測定**
   - 現状: 未実装
   - 方法: 上記「推測される解決策1」を実装
   - 目的: 呼び出し回数削減の効果確認

### 優先度: 中

3. **スレッド生成元の特定**
   - 現状: 未特定
   - 方法: スレッドダンプ、プロファイリング
   - 目的: 仮説Bの検証

4. **TypeScriptプロセスのメモリ監視**
   - 現状: 未測定
   - 方法: TypeScriptプロセスのRSS監視
   - 目的: 仮説Dの検証

### 優先度: 低

5. **PyArrow内部バッファの観測**
   - 現状: 観測方法不明
   - 方法: PyArrow内部APIの調査が必要
   - 目的: 仮説Cの検証

---

## 4. 論理的誤謬の自己修正

### 誤謬1: 早期の一般化
**誤り**: 100ms待機で効果なし → あらゆる待機戦略が無効
**修正**: 100ms待機は特定の実装であり、他の戦略（バッチ化等）は未検証

### 誤謬2: 相関と因果の混同
**誤り**: スレッド数増加とメモリ増加が相関 → スレッドがメモリ増加の原因
**修正**: 相関は観測されたが、因果関係は未検証（両方が別の原因の結果かもしれない）

### 誤謬3: delの効果の誤認
**誤り**: `del sections`で参照カウント0 → 即座にメモリ解放
**修正**: LanceDB内部に参照が残る場合、`del`は無効。効果があるのは`gc.collect()`

---

## 5. 次のアクション（提案）

### 即座に実行可能
1. LanceDB Fragment数の確認コードを書いて実行
2. add_sections()バッチ化の実装（TypeScript側）
3. バッチ化版でkarte-io-systemsテスト実行

### 追加調査が必要
4. スレッド生成元の特定（プロファイリングツール使用）
5. TypeScriptプロセスのメモリ監視スクリプト作成

### 判断保留
6. PyArrow内部バッファの調査（優先度低、調査コスト高）
