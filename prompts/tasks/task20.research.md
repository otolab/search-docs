cl-nagoya/ruri-v3-30m を使用した SentenceTransformer.encode 処理におけるメモリリークの診断と回避策に関する技術レポートI. エグゼクティブ・サマリーと統合推奨事項A. 問題の概要本レポートは、cl-nagoya/ruri-v3-30m のようなモデルを SentenceTransformer.encode メソッドで処理する際に発生する、重大なメモリリークの回避策を調査するものです。この問題は、大量のテキストデータ、または特に長いシーケンスを処理する際に、システムメモリ（RAM）が際限なく消費され、最終的にプロセスがクラッシュするという形で顕在化します。このリークは sentence-transformers ライブラリ自体に起因するものではなく、その下層でテキストのトークン化（字句解析）を担う huggingface/tokenizers ライブラリのRustバックエンドに存在する、より根深いバグの直接的な症状であることが特定されています 1。B. 診断の結論このメモリリークは、単一の単純なバグではなく、複数のシステムレベルの要因が相互に作用した結果発生する、複雑な相互運用性の不具合であると結論付けられます。主な要因は、huggingface/tokenizers のRustベースの並列処理機構（および関連する内部キャッシュ）と、Pythonの fork() ベースのマルチプロセッシング（datasets.map(num_proc=...) などで利用される）との間の深刻な競合です 3。さらに、この問題はPython 3.10以降のバージョンで顕著に悪化し、Python 3.8では観察されないことが報告されており、CPythonのメモリ管理の変更が関連していることを強く示唆しています 5。利用者が観測している現象は、HuggingFaceのIssue #1539（長い文字列のエンコード時のリーク）1 と #1495（マルチプロセッシング時のリーク）7 という、一見異なる二つの問題が同時にトリガーされた「パーフェクト・ストーム」である可能性が極めて高いです。C. 最優先推奨事項（即時対応）最も効果的かつ導入負荷の低い即時回避策は、tokenizers ライブラリのRustレベルでの並列処理を明示的に無効化することです。これは、Pythonスクリプトの実行開始時に、transformers や sentence_transformers を import する前に、以下の環境変数を設定することで達成できます 1。Pythonimport os
os.environ = "false"

# この後にライブラリをインポートする
from sentence_transformers import SentenceTransformer
#...
D. 副次的推奨事項（本番環境の安定化）最優先推奨事項でリークが完全に解消されない場合、またはより堅牢な本番環境を構築する場合には、以下の環境レベルでの対策を推奨します。Python 3.8へのダウングレード: このメモリリークはPython 3.10以降で発生し、Python 3.8.xでは観察されないという強力な証拠があります 5。実行環境（例：Dockerイメージ）を python:3.8-slim などにダウングレードすることは、非常に有効な根本対策となります。jemalloc の使用: 大規模なデータ処理を行うコンテナ環境において、システムのデフォルトメモリ アロケータ（glibc malloc）を、より堅牢で並行処理に強い jemalloc に切り替えることで、問題が解決したとの報告があります 1。これは LD_PRELOAD 環境変数経由で比較的容易に導入可能です。E. 効果のない対策一般的なPythonのメモリ管理テクニック、例えば gc.collect() の明示的な呼び出し、del 文によるオブジェクト削除、torch.cuda.empty_cache() の実行などは、この特定のメモリリークに対して全く効果がありません 1。これは、リークしているメモリがPythonインタプリタのガベージコレクション（GC）の管理下にあるPythonヒープではなく、Rustライブラリのランタイムによって確保・管理されているRustヒープ内に存在するためです。II. 診断分析: SentenceTransformer.encode メモリリークの根本原因A. ユーザーが直面する症状: SentenceTransformer のリーク (UKPLab/sentence-transformers#1795)利用者が観測している SentenceTransformer.encode でのメモリリークは、sentence-transformers ライブラリのGitHub Issue #1795（huggingface/tokenizers Issue #1539内で言及）で詳細に報告されている現象と一致します 1。SentenceTransformer.encode は、内部でバッチ単位のトークン化とモデル推論を行う高レベルなラッパーメソッドです。Issue #1795で提示された再現スクリプト（2 で引用）は、非常に示唆に富んでいます。Python#  からの再現コード
model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')
def random_string(length: int) -> str:
    return ''.join(random.choices(string.ascii_uppercase + string.digits, k=length))

for iteration in range(99999999):
    # 非常に長いランダム文字列 (12345文字) のバッチ (200件) をエンコード
    model.encode([random_string(12345) for _ in range(200)])
    memory_usage_in_MiB = psutil.Process().memory_info().rss / (1024 * 1024)
    print(f'{iteration},{memory_usage_in_MiB}', flush=True)
このスクリプトは、model.encode を非常に長い文字列（12345文字）のバッチ（200件）に対して繰り返し呼び出すと、反復ごとにメモリ使用量が線形に、際限なく増加し続ける（例：0回目で1329MB、50回目で5749MB）ことを明確に示しています 2。これは、cl-nagoya/ruri-v3-30m のようなモデルで大規模なドキュメントを処理する際の利用者のワークロードと完全に一致します。sentence-transformers ライブラリは、GPUスループットを最大化するためにデフォルトで比較大きなバッチサイズ（例：32）を使用する設計になっており 11、この設計思想が、下層にある tokenizers のバグの影響を特に受けやすくしています。B. 根本的な病理: huggingface/tokenizers Issues #1539 & #1495利用者が注目している2つのIssue (#1539と#1495) は、この問題の核心を突いています。これらは別個のバグではなく、同一の根本原因が異なる側面として現れたもの（二つの異なる症状）と分析されます。1. 症状1: 長い文字列とキャッシュのリーク (#1539)このIssueは「Memory leak for large strings」（長い文字列でのメモリリーク）と題されています 1。提供された再現スクリプト 1 は、use_fast=True（Rustベースの高速トークナイザ）を指定し、非常に長い文字列（例：s = f'{i} {i} ' * 10000）をループ内で繰り返し tokenizer.encode(s) で処理すると、メモリ使用量が「際限なく上昇し続ける」ことを示しています。この症状は、Rustバックエンドの内部キャッシュ機構 1 または文字列の受け渡し処理において、メモリが確保される一方で、適切に解放されていないことを示唆しています。2. 症状2: マルチプロセッシングと fork() のリーク (#1495)このIssueは「LLamaTokenizer... causing memory leak when used with multiprocessing / dataset.map(num_proc)」と題されています 7。この報告は、datasets ライブラリの dataset.map を num_proc > 1（例：num_proc=16）で実行すると、たとえ use_fast=False（Pythonベースのトークナイザ）を使ったとしても、メモリリークが発生することを示しています 7。use_fast=False でさえも、リークが解消されるわけではなく、OOM (Out of Memory) に至るまでの処理行数が約2,000から10,000に増えるだけで、根本的なリークは残存します 7。これは、問題が fork() というシステムコールとライブラリの内部状態との間に根本的な競合があることを示しています 3。C. 根本原因の統合分析: システムレベルの相互作用不全ここでの重要な分析結果は、利用者が直面している問題が、独立した二つのバグではなく、tokenizers のRustバックエンドが、Pythonのマルチプロセッシングモデルおよびメモリ管理モデルと相互作用する際の、単一の根深いシステムレベルの欠陥から生じる二つの異なる症状であるという点です。この分析は、以下の連鎖的な事実に基づいています。利用者の問題は SentenceTransformer.encode 2 で発生しており、これは長い文字列のバッチ処理を含みます。これは**Issue #1539（長い文字列のリーク）**の症状と一致します 1。cl-nagoya/ruri-v3-30m のような大規模コーパスを処理する際、SentenceTransformer.encode をマルチプロセッシング・プールや datasets.map(num_proc=...) を使って並列実行することは、標準的なベストプラクティスです 11。これは**Issue #1495（マルチプロセッシングのリーク）**の状況と一致します 7。したがって、利用者のワークロードは、両方のIssueを同時にトリガーする「パーフェクト・ストーム」となっている可能性が非常に高いです。決定的な証拠は、Issue #1539のスレッド内（1）にあります。あるユーザーが600GB以上のデータを encode_batch（#1539の症状）で前処理している際に、TOKENIZERS_PARALLELISM=false（#1495の回避策）を設定したところ、問題が解決したと報告しています。この最後の事実は、極めて重要です。「並列処理（Parallelism）を無効にする」ことが「キャッシュのリーク（Cache Leak）」を修正するという事実は、このキャッシュリーク（#1539）が単純なロジックエラーではなく、Rustの内部スレッドプールがアクティブな場合にのみトリガーされる競合状態（Race Condition）または内部状態の破損である可能性を強く示唆しています。スレッドプールを無効化（TOKENIZERS_PARALLELISM=false）することで、Rust側の処理がシリアル化（直列化）され、意図せずしてこの競合状態が回避され、結果として両方の問題が同時に解決される、と分析されます。III. 最優先の推奨回避策: TOKENIZERS_PARALLELISM=falseA. 「Rustの並列処理 vs Pythonの fork()」競合の技術的背景この回避策がなぜ機能するのかを理解することは、極めて重要です。この問題は、tokenizers ライブラリ（バージョン0.8.0以降）の設計と、Pythonの multiprocessing の仕組みとの間の根本的なミスマッチに起因します 3。Fast Tokenizersの内部: 高速トークナイザ（use_fast=True）は、その速度を達成するためにRustバックエンドに依存しています 15。このRustバックエンドは、内部で rayon のようなライブラリを使用し、暗黙的にグローバルなスレッドプールを初期化して処理を並列化します。Python Multiprocessingの仕組み: Pythonの multiprocessing ライブラリ（およびそれを呼び出す datasets.map(num_proc=...)）は、UNIX系システムにおいて os.fork() システムコールを使用して新しい子プロセスを作成します。競合の発生: fork() は、親プロセスのメモリ空間をそのまま子プロセスにコピーします。もし、fork() が呼び出される前に（例えば、スクリプトのグローバルスコープで）高速トークナイザが一度でも使用されると、Rustのグローバルスレッドプールが初期化されます。このスレッドプールの内部状態（ロック、ミューテックス、スレッドハンドルなど）が、そのまま子プロセスにコピーされます。結果: 子プロセスは、もはや機能しない（親プロセスに属していた）スレッドプールの「壊れた」状態を継承します。この状態で子プロセスがトークナイザを使用しようとすると、デッドロック（警告メッセージで示される主な懸念 3）や、今回観測されているようなメモリ状態の破損およびリークを引き起こします。TOKENIZERS_PARALLELISM=false 16 を設定することは、Rustバックエンドに対して「グローバルスレッドプールを一切初期化するな」と明示的に指示するものです。これにより、fork() 時にコピーされるべき危険な内部状態が存在しなくなり、この根本的な競合が完全に回避されます。B. 実装方法: 環境変数を設定する正しいタイミングこの回避策を機能させるには、環境変数を適切なタイミングで設定する必要があります。Rustライブラリが初期化される（つまりPythonで import される）前に設定されなければなりません。方法1: Pythonスクリプト内（推奨）最も堅牢な方法は、Pythonスクリプトの先頭、他のどの import 文よりも前に os.environ を設定することです 4。Pythonimport os
# 他のどのimportよりも先に、この行を記述する
os.environ = "false"

# これ以降にライブラリをインポートする
import torch
from transformers import AutoTokenizer
from sentence_transformers import SentenceTransformer

#... 以降のスクリプト...
方法2: シェル経由スクリプトを実行するシェルの環境変数として設定することも可能です 4。Bashexport TOKENIZERS_PARALLELISM=false
python your_encoding_script.py
transformers の引数として渡す（例：13 で試みられたが機能しなかった）など、ライブラリのインポート後に設定しても効果はありません。C. 有効性の証拠この TOKENIZERS_PARALLELISM=false の設定は、単なる理論上の回避策ではありません。Issue #1539のスレッドにおいて、600GB超のデータを encode_batch で前処理する際に発生したメモリリークを、この設定（または jemalloc の使用）によって「修正できるようだ (seems to fix the problem)」と、複数のユーザーが具体的に報告しています 1。また、sentence-transformers をクラウド環境で使用する際の ThreadPoolBuildError を回避するためにも、この設定が推奨されています 18。トレードオフとして、トークン化自体の並列処理が無効になるため、トークン化ステップが（理論上は）低速化する可能性があります。しかし、SentenceTransformer のワークロード全体で見れば、トークン化の時間はモデルの推論時間に比べてごくわずかであり、メモリリークを回避するメリットが遥かに上回ります。IV. 副次的および環境的回避策の分析TOKENIZERS_PARALLELISM=false が第一の推奨策ですが、状況によっては、より抜本的な環境レベルでの対策が有効、あるいは必要となる場合があります。A. 環境的対策1: Pythonバージョンのダウングレード (3.10+ → 3.8.x)このメモリリークに関する調査で、最も強力な手がかりの一つが、Pythonのバージョン依存性です。huggingface/tokenizers のIssue #1706 5 および transformers のIssue #35434 6 では、以下のシステム情報が明確に報告されています。Python 3.8.11: メモリリークは観察されない (No memory leak observed)Python 3.10.*: メモリリークが発生する (Memory leak occurs)これは、このリークが tokenizers ライブラリ内の単純なロジックエラーではなく、CPython 3.10以降のメモリ管理、fork() の実装、あるいはFFI (Foreign Function Interface) レイヤーの変更と、Rustバックエンドの（Python 3.8環境の挙動を前提としてコンパイルされた）バイナリとの間の、**システムレベルの非互換性または回帰（リグレッション）**であることを示す「決定的な証拠（Smoking Gun）」です。Rustバックエンドは、CPythonがどのようにメモリを管理し、FFI境界でどのようにデータを渡すかという仮定の上で動作します。Python 3.10での変更がこの仮定を破り、結果としてRust側で割り当てられたメモリがPython側から適切に解放されなくなる、というメカニズムが強く推察されます。したがって、実行環境（特にDockerイメージ）を python:3.8-slim のようなPython 3.8.xベースのものにダウングレードすることは、非常に強力で信頼性の高い回避策となります 19。B. 環境的対策2: システムレベルのメモリアロケータ (jemalloc)前述の通り、大規模データ（600GB+）の前処理において、TOKENIZERS_PARALLELISM=false と並んで「より堅牢なメモリアロケータ（jemalloc）を使用する」ことが問題を修正したと報告されています 1。これは、前述の「システムレベルの相互作用不全」という診断を裏付けるものです。標準的なLinux環境では、CPythonは glibc の malloc を使用します。Rustライブラリもまた、glibc にリンクしているか、独自のシステムアロケータを使用しています。fork() されたマルチスレッド環境で、二つの異なる高頻度のメモリアロケータ（Python側とRust側）が同時に動作すると、メモリの断片化（fragmentation）や競合（contention）が発生し、病的な動作（この場合はリーク）を引き起こす可能性があります。jemalloc は、特に高並行性（high-concurrency）と断片化の削減のために設計された、高性能な代替アロケータです。LD_PRELOAD=libjemalloc.so を使用すると、CPythonとRustライブラリの両方からのすべてのメモリアロケーションコールが、強制的に jemalloc を経由するようになります。これによりアロケータが統一され、競合の源が取り除かれ、結果としてリークが解消されると考えられます。この対策は高度ですが、LD_PRELOAD 環境変数を設定するだけで導入できるため、特にコンテナ化された本番環境では有効な選択肢となります。C. アプリケーションレベル対策1: Rustバックエンドの無効化 (use_fast=False)AutoTokenizer.from_pretrained 時に use_fast=False を指定することで、Rustベースの TokenizerFast ではなく、純粋なPythonベースの Tokenizer を使用するよう切り替えることができます 10。リークは use_fast=True で一貫して再現されています 1。しかし、この対策には重大な欠陥があります。Issue #1495（マルチプロセッシング）の報告では、use_fast=False はリークを修正しませんでした。OOM (Out of Memory) に至るまでの時間を延長する（2,000行から10,000行に遅らせる）だけで、根本的なリークは残存します 7。さらに、Rustバックエンドの速度という最大の利点を放棄することになり、トークン化ステップが大幅なボトルネックとなります。したがって、これは信頼できる修正策ではなく、生産環境での解決策としては推奨されません。D. アプリケーションレベル対策2: 定期的なトークナイザの再初期化Issue #1539（長い文字列）の再現スクリプト 1 は、興味深い回避策を提示しています。refresh_every = 100（100反復ごとにトークナイザを再初期化）に設定するとメモリ使用量が安定し、refresh_every = 100000（事実上再初期化しない）に設定するとメモリが際限なく増加する、というものです 1。これは、リークがRustの内部状態（キャッシュ）にあり、tokenizer = AutoTokenizer.from_pretrained(...) を再実行することが、PythonレベルからRustバックエンドのデストラクタを呼び出し、破損したキャッシュを含むすべての内部メモリを強制的に解放する唯一の方法であることを示しています。しかし、この方法は SentenceTransformer.encode の内部で実行することはできず、ループの外側で適用するしかありません。トークナイザの再読み込みはオーバーヘッドが大きく、非実用的な「力ずく」のパッチであり、利用者の問題に対する推奨される解決策ではありません。V. 効果のない試みの解明: なぜ gc.collect() は失敗するのかA. 問題: リークしたメモリはPythonの管理外にあるメモリリークに直面した際、多くの開発者が本能的にPythonの標準的なメモリクリーンアップ技術を試みます。これには、del obj によるオブジェクトの削除、gc.collect() による強制ガベージコレクション、torch.cuda.empty_cache() によるPyTorchのGPUキャッシュクリアが含まれます 9。しかし、今回のリークに関しては、これらの試みはすべて効果がないと報告されています 1。B. なぜ失敗するのか: Rustヒープ vs Pythonヒープgc.collect() が失敗し、前述の「定期的な再初期化」（IV-D）が（非実用的ではあるが）機能するという事実は、リークがPythonが管理するメモリではなく、Rustが管理するメモリで発生していることを決定的に証明しています。Python GCの役割: gc.collect() は、Pythonのガベージコレクタを起動します。このGCは、Pythonオブジェクト（refcount が0になったものなど）のために確保されたPythonヒープ上のメモリのみを追跡し、解放します。torch.cuda.empty_cache() の役割: この関数は、PyTorchがGPU上で確保したメモリブロックのうち、現在は使用されていないが将来のために予約されているキャッシュを解放するだけで、CPUのRAMには影響しません。Rustヒープのリーク: tokenizers ライブラリ 15 でのリークは、Rustバックエンドが文字列やキャッシュエントリのためにRustヒープ内にメモリを割り当てた際に発生します。このメモリはPythonのGCの追跡対象外です。何らかのバグ（例：前述の競合状態）により、Rustコードがこのメモリを drop（解放）しそこねています。Pythonからの不可視性: Pythonインタプリタから見ると、tokenizer オブジェクト自体は、依然として一定のサイズの単一のPythonオブジェクトです。Pythonのメモリプロファイラは tokenizer オブジェクトそのものしか見えず、そのRust実装の内部でリークしているギガバイト単位のメモリを認識できません。したがって、gc.collect() を呼び出しても、Pythonは解放すべきガベージを（Pythonヒープ内に）見つけられないため、何も起こりません。PythonレベルからこのリークしたRustメモリを解放する唯一の方法は、tokenizer オブジェクト自体を del し、その参照カウントを0にすることです。これにより、最終的にRustオブジェクトのメインデストラクタが呼び出され、関連するRustヒープ全体が（破損したキャッシュごと）解放されます。これが「定期的な再初期化」が機能する理由ですが、根本的な解決策ではありません。利用者は、gc.collect() のような効果のない対策に時間を費やすべきではありません。VI. cl-nagoya/ruri-v3-30m 処理のための段階的実施ガイドこのセクションでは、cl-nagoya/ruri-v3-30m を使用した SentenceTransformer.encode のワークロードを安定させるための、具体的な行動計画を提示します。まず、利用可能な回避策のトレードオフを比較検討します。A. 表1: 回避策の比較分析以下の表は、分析された主要な回避策を、その有効性、パフォーマンスへの影響、および導入の複雑さの観点からまとめたものです。回避策有効性（リーク修正）パフォーマンス影響（速度）実装の複雑さスコープと（関連ソース）TOKENIZERS_PARALLELISM="false"高（実証済み）低～無視可能（トークン化は僅かに低速化するが、モデル推論には影響なし）非常に低い（コード1行またはシェルコマンド1つ）アプリケーション / 環境 1Python 3.8.xへのダウングレード高（実証済み）なし高い（完全な環境再構築、例: 新規Dockerイメージが必要）環境 5jemalloc アロケータの使用高（実証済み）なし（パフォーマンスが向上する可能性もある）中（libjemalloc のインストールと LD_PRELOAD 変数が必要）システム / 環境 1定期的なトークナイザ再初期化高（リーク停止）非常に高い（負）（トークナイザ再読み込みによるI/Oオーバーヘッドが常時発生）中（encode ループのリファクタリングが必要で、SentenceTransformer では困難）アプリケーション（パッチ） 1use_fast=False低（リークを遅らせるだけで、修正しない）高い（負）（Pythonベースのトークン化により大幅に低速化）非常に低い（from_pretrained の引数1つ）アプリケーション 7gc.collect()なしなし（僅かなオーバーヘッドが追加される可能性）非常に低い無効 1B. フェーズ1: 即時緩和策 (Must-Do)アクション: TOKENIZERS_PARALLELISM="false" の回避策を適用します。実装: メインのPythonスクリプトの最上部、他のどの import 文よりも前に、import os; os.environ = "false" の2行を追加します。根拠: これは最も労力が少なく、最も成功確率の高い修正策です。fork() との競合という根本原因 4 に直接対処し、同時に長い文字列のキャッシュリークも修正することが報告されています 1。直ちにテストすることを推奨します。C. フェーズ2: 安定した長期解決策 (Should-Do)アクション: フェーズ1で問題が解決しない場合、あるいはこのタスクのために安定した新しい本番環境を構築する場合は、Python 3.8.xベースのDockerイメージ（例: python:3.8-slim）を作成します。実装: Dockerfile を変更し、Python 3.8のベースイメージを使用し、すべての依存関係を再インストールします。根拠: このリークがPython 3.10+固有のものであるという証拠は強力です 5。Python 3.8に戻すことで、バグを可能にする環境変数（CPythonのメモリ管理の変更）が取り除かれます。これは、TOKENIZERS_PARALLELISM フラグよりも堅牢で体系的な修正策です。D. フェーズ3: 本番環境の堅牢化 (Could-Do)アクション: メモリの安定性が最重要視される最大規模のコンテナ化されたワークロード（例: Kubernetes上）では、フェーズ2のソリューションと jemalloc アロケータを組み合わせます。実装:Dockerfile 内: RUN apt-get update && apt-get install -y libjemalloc-devコンテナのランタイム環境（docker run コマンドやKubernetesマニフェスト）: 環境変数 LD_PRELOAD="/usr/lib/x86_64-linux-gnu/libjemalloc.so.2" を設定します（パスはディストリビューションによって異なる場合があります）。根拠: これにより、二重の防御が提供されます。Python 3.8環境がCPythonレベルの非互換性を回避し、同時に jemalloc がより堅牢で並行処理に適したシステムアロケータを提供し、最下層での断片化や競合の問題を解決します 1。この組み合わせが、診断された問題に対する最も回復力の高いソリューションとなります。